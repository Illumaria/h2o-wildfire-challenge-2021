{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cebefeb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171bfc3-5047-4c18-92f0-e6822f28dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6305fbc6-1c62-446f-bb12-5541946da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/roelbertens/time-series-nested-cv/blob/master/time_series_cross_validation/custom_time_series_split.py\n",
    "\n",
    "class CustomTimeSeriesSplit:\n",
    "    def __init__(self,\n",
    "                 train_set_size: int,\n",
    "                 test_set_size: int\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param train_set_size: data points (days) in each fold for the train set\n",
    "        :param test_set_size: data points (days) in each fold for the test set\n",
    "        \"\"\"\n",
    "        self.train_set_size = train_set_size\n",
    "        self.test_set_size = test_set_size\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "\n",
    "    def split(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return train/test split indices.\n",
    "        :param x: time series to use for prediction, shape (n_samples, n_features)\n",
    "        :param y: time series to predict, shape (n_samples, n_features)\n",
    "        :return: (train_indices, test_indices)\n",
    "        Note: index of both x and y should be of type datetime.\n",
    "        \"\"\"\n",
    "        if y is not None:\n",
    "            assert x.index.equals(y.index)\n",
    "        split_points = self.get_split_points(x)\n",
    "        for split_point in split_points:\n",
    "            is_train = (x.index < split_point) & (x.index >= split_point -\n",
    "                                                  pd.Timedelta(self.train_set_size, unit=\"D\"))\n",
    "            is_test = (x.index >= split_point) & (x.index < split_point +\n",
    "                                                  pd.Timedelta(self.test_set_size, unit=\"D\"))\n",
    "            if not is_train.any() or not is_test.any():\n",
    "                self._logger.warning(\n",
    "                    \"Found %d train and %d test observations \"\n",
    "                    \"skipping fold for split point %s\",\n",
    "                    is_train.sum(), is_test.sum(), split_point\n",
    "                )\n",
    "                continue\n",
    "            dummy_ix = pd.Series(range(0, len(x)), index=x.index)\n",
    "            ix_train = dummy_ix.loc[is_train].values\n",
    "            ix_test = dummy_ix.loc[is_test].values\n",
    "            if ix_train is None or ix_test is None:\n",
    "                self._logger.warning(\n",
    "                    \"Found no data for train or test period, \"\n",
    "                    \"skipping fold for split date %s\",\n",
    "                    split_point\n",
    "                )\n",
    "                continue\n",
    "            yield ix_train, ix_test\n",
    "\n",
    "    def get_split_points(self, x: np.array) -> pd.DatetimeIndex:\n",
    "        \"\"\"Get all possible split point dates\"\"\"\n",
    "        start = x.index.min() + pd.Timedelta(self.train_set_size, unit=\"D\")\n",
    "        end = x.index.max() - pd.Timedelta(self.test_set_size - 1, unit=\"D\")\n",
    "        self._logger.info(f\"Generating split points from {start} to {end}\")\n",
    "        split_range = pd.date_range(start, end, freq=\"D\")\n",
    "        first_split_point =  (len(split_range) + self.test_set_size - 1) % self.test_set_size\n",
    "        return split_range[first_split_point::self.test_set_size]\n",
    "\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self, df, target, feats, cat_feats):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.feats = feats\n",
    "        self.cat_feats = cat_feats\n",
    "        self.mode = \"classification\" if type(target) == str else \"multiclassification\"\n",
    "\n",
    "    def train_folds(self, train_size=120, test_size=30, iterations=1000, early_stopping=False):\n",
    "        if self.mode == \"classification\":\n",
    "            oof_preds = np.zeros(self.df.shape[0])\n",
    "        else:\n",
    "            oof_preds = np.zeros((self.df.shape[0], len(targets)))\n",
    "\n",
    "        folds_mask = np.zeros(oof_preds.shape[0])\n",
    "        for fold_, (train_index, test_index) in enumerate(CustomTimeSeriesSplit(train_set_size=train_size, test_set_size=test_size).split(self.df)):\n",
    "            X_train, y_train = self.df.iloc[train_index, :][self.feats], self.df.iloc[train_index, :][self.target]\n",
    "            X_val, y_val = self.df.iloc[test_index, :][self.feats], self.df.iloc[test_index, :][self.target]\n",
    "\n",
    "            weeks_train = X_train.reset_index()[\"dt\"]\n",
    "            weeks_test = X_val.reset_index()[\"dt\"]\n",
    "\n",
    "            tr_start_week = weeks_train.min()\n",
    "            tr_end_week = weeks_train.max()\n",
    "            ts_start_week = weeks_test.min()\n",
    "            ts_end_week = weeks_test.max()\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(f\"Fold {fold_} train ({tr_start_week}, {tr_end_week}) test ({ts_start_week}, {ts_end_week})\")\n",
    "\n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function=\"Logloss\" if self.mode==\"classification\" else \"MultiLogloss\",\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric=\"F1\" if self.mode==\"classification\" else \"MultiLogloss\", \n",
    "                task_type=\"CPU\",\n",
    "                early_stopping_rounds=100,\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "\n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "            D_val = Pool(X_val, y_val, cat_features=cat_feats, feature_names=feats)\n",
    "            \n",
    "            print(\"Train catboost\")\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=D_val if early_stopping else None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            if self.mode == \"classification\":\n",
    "                D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "                D_val_lgb = lgb.Dataset(X_val, y_val, weight=None, free_raw_data=False)\n",
    "                \n",
    "                print(\"Train lgbm\")\n",
    "                lgbm_model = lgb.train(\n",
    "                    {\n",
    "                        \"objective\": \"binary\",\n",
    "                        \"feature_pre_filter\": False,\n",
    "                        \"lambda_l1\": 5.246525412521277e-08,\n",
    "                        \"lambda_l2\": 3.963188589061798e-05,\n",
    "                        \"num_leaves\": 6,\n",
    "                        \"feature_fraction\": 0.7,\n",
    "                        \"bagging_fraction\": 1.0,\n",
    "                        \"bagging_freq\": 0,\n",
    "                        \"min_child_samples\": 20,\n",
    "                    },\n",
    "                    D_train_lgb,\n",
    "                    num_boost_round=iterations,\n",
    "                    early_stopping_rounds=200 if early_stopping else None,\n",
    "                    valid_sets=D_val_lgb if early_stopping else None,\n",
    "                    feature_name=feats,\n",
    "                    verbose_eval=500,\n",
    "                )\n",
    "                preds = (cat_model.predict_proba(X_val)[:, 1] + lgbm_model.predict(X_val)) / 2\n",
    "                print()\n",
    "                print(f\"Fold {fold_} F1 Score \", metrics.f1_score(y_val, preds.round()))\n",
    "                print(f\"Fold {fold_} ROC AUC Score \", metrics.roc_auc_score(y_val, preds.round()))\n",
    "                print(f\"Fold {fold_} Confusion matrix\")\n",
    "                print(metrics.confusion_matrix(y_val, preds.round()))\n",
    "                oof_preds[test_index] = preds\n",
    "            else:\n",
    "                oof_preds[test_index] = cat_model.predict(X_val)\n",
    "                print(f\"Fold {fold_} F1 Score \", metrics.f1_score(y_val, oof_preds[test_index].round(), average=\"micro\"))\n",
    "                try:\n",
    "                    print(f\"Fold {fold_} ROC AUC Score \", metrics.roc_auc_score(y_val, oof_preds[test_index]))\n",
    "                except ValueError:\n",
    "                    print(f\"Fold {fold_} ROC AUC Score \", 0)\n",
    "                    \n",
    "            folds_mask[test_index] = 1\n",
    "        \n",
    "        if self.mode == \"classification\":\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1].round(), average=\"micro\")\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1], average=\"micro\")\n",
    "        else:\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1].round(), average=\"micro\")\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1], average=\"micro\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Overall OOF F1 Micro \", oof_f1micro)\n",
    "        print(\"Overall OOF Mean ROC AUC Score \", oof_f1micro)\n",
    "        \n",
    "    def train_final_models(self, iterations=1000, early_stopping=False):\n",
    "        if self.mode == \"classification\":\n",
    "            X_train, y_train = self.df.iloc[:, :][self.feats], self.df.iloc[:, :][self.target]\n",
    "\n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function=\"Logloss\",\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric=\"F1\", \n",
    "                task_type=\"CPU\",\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "\n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "\n",
    "            print(\"Train catboost\")\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "\n",
    "            D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "\n",
    "            print(\"Train lgbm\")\n",
    "            lgbm_model = lgb.train(\n",
    "                {\n",
    "                    \"objective\": \"binary\",\n",
    "                    \"feature_pre_filter\": False,\n",
    "                    \"lambda_l1\": 5.246525412521277e-08,\n",
    "                    \"lambda_l2\": 3.963188589061798e-05,\n",
    "                    \"num_leaves\": 6,\n",
    "                    \"feature_fraction\": 0.7,\n",
    "                    \"bagging_fraction\": 1.0,\n",
    "                    \"bagging_freq\": 0,\n",
    "                    \"min_child_samples\": 20,\n",
    "                },\n",
    "                D_train_lgb,\n",
    "                num_boost_round=iterations,\n",
    "                valid_sets=None,\n",
    "                feature_name=feats,\n",
    "                verbose_eval=500,\n",
    "            )\n",
    "\n",
    "            return cat_model, lgbm_model\n",
    "\n",
    "        elif self.mode == \"multiclassification\":\n",
    "            raise NotImplementedError "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e204436",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0cf34c8-cb1a-4da4-b3a8-f6068764ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920fb9f3-d1d1-4bd8-af86-e80af9986685",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"SATELLITE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a44b8dd-ecfc-4eac-969f-f4261f9ae3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CONFIDENCE\"] = df[\"CONFIDENCE\"].map({\"l\":0, \"h\":1, \"n\":3})\n",
    "df[\"SATELLITE\"] = df[\"SATELLITE\"].map({\"1\":0, \"N\":1})\n",
    "df[\"DAYNIGHT\"] = df[\"DAYNIGHT\"].map({\"D\":0, \"N\":1})\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"]).dt.date\n",
    "df = df.set_index(\"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e8e44-600b-4bba-86ce-d1bc33ce7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"infire_day_1\",\"infire_day_2\",\"infire_day_3\",\"infire_day_4\",\"infire_day_5\",\"infire_day_6\",\"infire_day_7\",\"infire_day_8\"]\n",
    "feats = [\"LATITUDE\",\"LONGITUDE\",\"BRIGHTNESS\",\"SCAN\",\"TRACK\",\"ACQ_TIME\",\"SATELLITE\",\"DAYNIGHT\",\"CONFIDENCE\",\"BRIGHT_T31\",\"FRP\"]\n",
    "#cat_feats = [\"grid_index\", \"DAYNIGHT\",\"SATELLITE\"]\n",
    "cat_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dbbe9e-2c7a-4807-a527-58496c20fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"infire_day_1\",\"infire_day_2\",\"infire_day_3\",\"infire_day_4\",\"infire_day_5\",\"infire_day_6\",\"infire_day_7\",\"infire_day_8\"]\n",
    "df[\"target\"] = (df[targets].sum(axis=1)>0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3262f9ab-ea43-4586-860e-a63fc1fc9abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4041550-5666-488b-896e-4b507915a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### syntetic data\n",
    "DROPOUT_PROBA = 0.7\n",
    "UPSAMPLE_RATE = 6\n",
    "\n",
    "df_syn_base = df[df[\"target\"]==0][feats]\n",
    "df_syn_final = pd.DataFrame()\n",
    "\n",
    "for i in range(UPSAMPLE_RATE):\n",
    "    df_syn = df_syn_base.copy()\n",
    "    for f in feats[3:]:\n",
    "        df_syn[f] = df_syn[f].apply(lambda x: x if np.random.random()>DROPOUT_PROBA else None).sample(frac=1.0).values\n",
    "    df_syn_final = pd.concat([df_syn_final, df_syn], axis=0)\n",
    "\n",
    "df_syn_final[\"target\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf840d-14fd-41c7-9427-293e74b44ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    df[feats+[\"target\"]],\n",
    "    df_syn_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a19926-c981-4665-8a5c-1dc87a8d6e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb3c8-1762-4498-982e-d8c330fc80b9",
   "metadata": {},
   "source": [
    "## Train with single lable (will we see fire during a period of 8 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a678e62-74c5-4bb9-bfd7-917c16915c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_model = ModelBuilder(df_combined, \"target\", feats, cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca9e17-61d9-4fc5-a84b-e82553df3b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_model.train_folds(train_size=120, test_size=30, iterations=1000, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb645a6f-5d16-49d0-a1ab-57b183e1fad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model, lgbm_model = fire_model.train_final_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca59767",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5890e0fb-4ed2-4fef-b7c3-eecdbe757160",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_model.save_model(\"catboost\", format=\"cbm\")\n",
    "\n",
    "lgbm_model.save_model(\"light_gbm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023a313-9d70-4142-94eb-0a068d82acce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
