{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9cebefeb",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f171bfc3-5047-4c18-92f0-e6822f28dfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6305fbc6-1c62-446f-bb12-5541946da97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/roelbertens/time-series-nested-cv/blob/master/time_series_cross_validation/custom_time_series_split.py\n",
    "\n",
    "class CustomTimeSeriesSplit:\n",
    "    def __init__(self,\n",
    "                 train_set_size: int,\n",
    "                 test_set_size: int\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        :param train_set_size: data points (days) in each fold for the train set\n",
    "        :param test_set_size: data points (days) in each fold for the test set\n",
    "        \"\"\"\n",
    "        self.train_set_size = train_set_size\n",
    "        self.test_set_size = test_set_size\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "\n",
    "    def split(\n",
    "        self,\n",
    "        x: np.ndarray,\n",
    "        y: Optional[np.ndarray] = None\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"Return train/test split indices.\n",
    "        :param x: time series to use for prediction, shape (n_samples, n_features)\n",
    "        :param y: time series to predict, shape (n_samples, n_features)\n",
    "        :return: (train_indices, test_indices)\n",
    "        Note: index of both x and y should be of type datetime.\n",
    "        \"\"\"\n",
    "        if y is not None:\n",
    "            assert x.index.equals(y.index)\n",
    "        split_points = self.get_split_points(x)\n",
    "        for split_point in split_points:\n",
    "            is_train = (x.index < split_point) & (x.index >= split_point -\n",
    "                                                  pd.Timedelta(self.train_set_size, unit=\"D\"))\n",
    "            is_test = (x.index >= split_point) & (x.index < split_point +\n",
    "                                                  pd.Timedelta(self.test_set_size, unit=\"D\"))\n",
    "            if not is_train.any() or not is_test.any():\n",
    "                self._logger.warning(\n",
    "                    \"Found %d train and %d test observations \"\n",
    "                    \"skipping fold for split point %s\",\n",
    "                    is_train.sum(), is_test.sum(), split_point\n",
    "                )\n",
    "                continue\n",
    "            dummy_ix = pd.Series(range(0, len(x)), index=x.index)\n",
    "            ix_train = dummy_ix.loc[is_train].values\n",
    "            ix_test = dummy_ix.loc[is_test].values\n",
    "            if ix_train is None or ix_test is None:\n",
    "                self._logger.warning(\n",
    "                    \"Found no data for train or test period, \"\n",
    "                    \"skipping fold for split date %s\",\n",
    "                    split_point\n",
    "                )\n",
    "                continue\n",
    "            yield ix_train, ix_test\n",
    "\n",
    "    def get_split_points(self, x: np.array) -> pd.DatetimeIndex:\n",
    "        \"\"\"Get all possible split point dates\"\"\"\n",
    "        start = x.index.min() + pd.Timedelta(self.train_set_size, unit=\"D\")\n",
    "        end = x.index.max() - pd.Timedelta(self.test_set_size - 1, unit=\"D\")\n",
    "        self._logger.info(f\"Generating split points from {start} to {end}\")\n",
    "        split_range = pd.date_range(start, end, freq=\"D\")\n",
    "        first_split_point =  (len(split_range) + self.test_set_size - 1) % self.test_set_size\n",
    "        return split_range[first_split_point::self.test_set_size]\n",
    "\n",
    "\n",
    "class ModelBuilder:\n",
    "    def __init__(self, df, target, feats, cat_feats):\n",
    "        self.df = df\n",
    "        self.target = target\n",
    "        self.feats = feats\n",
    "        self.cat_feats = cat_feats\n",
    "        self.mode = \"classification\" if type(target) == str else \"multiclassification\"\n",
    "\n",
    "    def train_folds(self, train_size=120, test_size=30, iterations=1000, early_stopping=False):\n",
    "        if self.mode == \"classification\":\n",
    "            oof_preds = np.zeros(self.df.shape[0])\n",
    "        else:\n",
    "            oof_preds = np.zeros((self.df.shape[0], len(targets)))\n",
    "\n",
    "        folds_mask = np.zeros(oof_preds.shape[0])\n",
    "        for fold_, (train_index, test_index) in enumerate(CustomTimeSeriesSplit(train_set_size=train_size, test_set_size=test_size).split(self.df)):\n",
    "            X_train, y_train = self.df.iloc[train_index, :][self.feats], self.df.iloc[train_index, :][self.target]\n",
    "            X_val, y_val = self.df.iloc[test_index, :][self.feats], self.df.iloc[test_index, :][self.target]\n",
    "\n",
    "            weeks_train = X_train.reset_index()[\"dt\"]\n",
    "            weeks_test = X_val.reset_index()[\"dt\"]\n",
    "\n",
    "            tr_start_week = weeks_train.min()\n",
    "            tr_end_week = weeks_train.max()\n",
    "            ts_start_week = weeks_test.min()\n",
    "            ts_end_week = weeks_test.max()\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(f\"Fold {fold_} train ({tr_start_week}, {tr_end_week}) test ({ts_start_week}, {ts_end_week})\")\n",
    "\n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function=\"Logloss\" if self.mode==\"classification\" else \"MultiLogloss\",\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric=\"F1\" if self.mode==\"classification\" else \"MultiLogloss\", \n",
    "                task_type=\"CPU\",\n",
    "                early_stopping_rounds=100,\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "\n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "            D_val = Pool(X_val, y_val, cat_features=cat_feats, feature_names=feats)\n",
    "            \n",
    "            print(\"Train catboost\")\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=D_val if early_stopping else None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "            \n",
    "            if self.mode == \"classification\":\n",
    "                D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "                D_val_lgb = lgb.Dataset(X_val, y_val, weight=None, free_raw_data=False)\n",
    "                \n",
    "                print(\"Train lgbm\")\n",
    "                lgbm_model = lgb.train(\n",
    "                    {\n",
    "                        \"objective\": \"binary\",\n",
    "                        \"feature_pre_filter\": False,\n",
    "                        \"lambda_l1\": 5.246525412521277e-08,\n",
    "                        \"lambda_l2\": 3.963188589061798e-05,\n",
    "                        \"num_leaves\": 6,\n",
    "                        \"feature_fraction\": 0.7,\n",
    "                        \"bagging_fraction\": 1.0,\n",
    "                        \"bagging_freq\": 0,\n",
    "                        \"min_child_samples\": 20,\n",
    "                    },\n",
    "                    D_train_lgb,\n",
    "                    num_boost_round=iterations,\n",
    "                    early_stopping_rounds=200 if early_stopping else None,\n",
    "                    valid_sets=D_val_lgb if early_stopping else None,\n",
    "                    feature_name=feats,\n",
    "                    verbose_eval=500,\n",
    "                )\n",
    "                preds = (cat_model.predict_proba(X_val)[:, 1] + lgbm_model.predict(X_val)) / 2\n",
    "                print()\n",
    "                print(f\"Fold {fold_} F1 Score \", metrics.f1_score(y_val, preds.round()))\n",
    "                print(f\"Fold {fold_} ROC AUC Score \", metrics.roc_auc_score(y_val, preds.round()))\n",
    "                print(f\"Fold {fold_} Confusion matrix\")\n",
    "                print(metrics.confusion_matrix(y_val, preds.round()))\n",
    "                oof_preds[test_index] = preds\n",
    "            else:\n",
    "                oof_preds[test_index] = cat_model.predict(X_val)\n",
    "                print(f\"Fold {fold_} F1 Score \", metrics.f1_score(y_val, oof_preds[test_index].round(), average=\"micro\"))\n",
    "                try:\n",
    "                    print(f\"Fold {fold_} ROC AUC Score \", metrics.roc_auc_score(y_val, oof_preds[test_index]))\n",
    "                except ValueError:\n",
    "                    print(f\"Fold {fold_} ROC AUC Score \", 0)\n",
    "                    \n",
    "            folds_mask[test_index] = 1\n",
    "        \n",
    "        if self.mode == \"classification\":\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1].round(), average=\"micro\")\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1], average=\"micro\")\n",
    "        else:\n",
    "            oof_f1micro = metrics.f1_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1].round(), average=\"micro\")\n",
    "            oof_f1micro = metrics.roc_auc_score(self.df.iloc[folds_mask == 1, :][self.target], oof_preds[folds_mask == 1], average=\"micro\")\n",
    "        \n",
    "        print()\n",
    "        print(\"Overall OOF F1 Micro \", oof_f1micro)\n",
    "        print(\"Overall OOF Mean ROC AUC Score \", oof_f1micro)\n",
    "        \n",
    "    def train_final_models(self, iterations=1000, early_stopping=False):\n",
    "        if self.mode == \"classification\":\n",
    "            X_train, y_train = self.df.iloc[:, :][self.feats], self.df.iloc[:, :][self.target]\n",
    "\n",
    "            cat_model = CatBoostClassifier(\n",
    "                iterations=iterations,\n",
    "                learning_rate=0.05,\n",
    "                metric_period=500,\n",
    "                loss_function=\"Logloss\",\n",
    "                l2_leaf_reg=10,\n",
    "                eval_metric=\"F1\", \n",
    "                task_type=\"CPU\",\n",
    "                random_seed=1234,\n",
    "                use_best_model=early_stopping\n",
    "                )\n",
    "\n",
    "            D_train = Pool(X_train, y_train, cat_features=cat_feats, feature_names=feats)\n",
    "\n",
    "            print(\"Train catboost\")\n",
    "            cat_model.fit(\n",
    "                D_train, \n",
    "                eval_set=None,\n",
    "                verbose=True,\n",
    "                plot=False\n",
    "            )\n",
    "\n",
    "            D_train_lgb = lgb.Dataset(X_train, y_train, weight=None, free_raw_data=False)\n",
    "\n",
    "            print(\"Train lgbm\")\n",
    "            lgbm_model = lgb.train(\n",
    "                {\n",
    "                    \"objective\": \"binary\",\n",
    "                    \"feature_pre_filter\": False,\n",
    "                    \"lambda_l1\": 5.246525412521277e-08,\n",
    "                    \"lambda_l2\": 3.963188589061798e-05,\n",
    "                    \"num_leaves\": 6,\n",
    "                    \"feature_fraction\": 0.7,\n",
    "                    \"bagging_fraction\": 1.0,\n",
    "                    \"bagging_freq\": 0,\n",
    "                    \"min_child_samples\": 20,\n",
    "                },\n",
    "                D_train_lgb,\n",
    "                num_boost_round=iterations,\n",
    "                valid_sets=None,\n",
    "                feature_name=feats,\n",
    "                verbose_eval=500,\n",
    "            )\n",
    "\n",
    "            return cat_model, lgbm_model\n",
    "\n",
    "        elif self.mode == \"multiclassification\":\n",
    "            raise NotImplementedError "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e204436",
   "metadata": {},
   "source": [
    "## Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0cf34c8-cb1a-4da4-b3a8-f6068764ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"merged.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "920fb9f3-d1d1-4bd8-af86-e80af9986685",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Terra', 'Aqua'], dtype=object)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"SATELLITE\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a44b8dd-ecfc-4eac-969f-f4261f9ae3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CONFIDENCE\"] = df[\"CONFIDENCE\"].map({\"l\":0, \"h\":1, \"n\":3})\n",
    "df[\"SATELLITE\"] = df[\"SATELLITE\"].map({\"1\":0, \"N\":1})\n",
    "df[\"DAYNIGHT\"] = df[\"DAYNIGHT\"].map({\"D\":0, \"N\":1})\n",
    "df[\"dt\"] = pd.to_datetime(df[\"dt\"]).dt.date\n",
    "df = df.set_index(\"dt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f17e8e44-600b-4bba-86ce-d1bc33ce7a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"infire_day_1\",\"infire_day_2\",\"infire_day_3\",\"infire_day_4\",\"infire_day_5\",\"infire_day_6\",\"infire_day_7\",\"infire_day_8\"]\n",
    "feats = [\"BRIGHTNESS\",\"SCAN\",\"TRACK\",\"ACQ_TIME\",\"SATELLITE\",\"DAYNIGHT\",\"CONFIDENCE\",\"BRIGHT_T31\",\"FRP\"]\n",
    "#cat_feats = [\"grid_index\", \"DAYNIGHT\",\"SATELLITE\"]\n",
    "cat_feats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63dbbe9e-2c7a-4807-a527-58496c20fd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [\"infire_day_1\",\"infire_day_2\",\"infire_day_3\",\"infire_day_4\",\"infire_day_5\",\"infire_day_6\",\"infire_day_7\",\"infire_day_8\"]\n",
    "df[\"target\"] = (df[targets].sum(axis=1)>0).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3262f9ab-ea43-4586-860e-a63fc1fc9abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.852313\n",
       "0    0.147687\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f4041550-5666-488b-896e-4b507915a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### syntetic data\n",
    "DROPOUT_PROBA = 0.7\n",
    "UPSAMPLE_RATE = 6\n",
    "\n",
    "df_syn_base = df[df[\"target\"]==0][feats]\n",
    "df_syn_final = pd.DataFrame()\n",
    "\n",
    "for i in range(UPSAMPLE_RATE):\n",
    "    df_syn = df_syn_base.copy()\n",
    "    for f in feats[3:]:\n",
    "        df_syn[f] = df_syn[f].apply(lambda x: x if np.random.random()>DROPOUT_PROBA else None).sample(frac=1.0).values\n",
    "    df_syn_final = pd.concat([df_syn_final, df_syn], axis=0)\n",
    "\n",
    "df_syn_final[\"target\"] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acbf840d-14fd-41c7-9427-293e74b44ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.concat([\n",
    "    df[feats+[\"target\"]],\n",
    "    df_syn_final], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "82a19926-c981-4665-8a5c-1dc87a8d6e8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.548113\n",
       "1    0.451887\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined[\"target\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7bb3c8-1762-4498-982e-d8c330fc80b9",
   "metadata": {},
   "source": [
    "## Train with single lable (will we see fire during a period of 8 days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a678e62-74c5-4bb9-bfd7-917c16915c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fire_model = ModelBuilder(df_combined, \"target\", feats, cat_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5ca9e17-61d9-4fc5-a84b-e82553df3b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fold 0 train (2020-02-20, 2020-06-18) test (2020-06-19, 2020-07-18)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9088065\ttotal: 58.8ms\tremaining: 58.7s\n",
      "500:\tlearn: 0.9158019\ttotal: 1.32s\tremaining: 1.32s\n",
      "999:\tlearn: 0.9267353\ttotal: 2.57s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 6978, number of negative: 9289\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000415 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 16267, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.428967 -> initscore=-0.286069\n",
      "[LightGBM] [Info] Start training from score -0.286069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 0 F1 Score  0.8901392860437228\n",
      "Fold 0 ROC AUC Score  0.9044177170846646\n",
      "Fold 0 Confusion matrix\n",
      "[[5731  926]\n",
      " [ 265 4825]]\n",
      "\n",
      "\n",
      "Fold 1 train (2020-03-21, 2020-07-18) test (2020-07-19, 2020-08-17)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9029634\ttotal: 3.1ms\tremaining: 3.09s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9144307\ttotal: 1.48s\tremaining: 1.47s\n",
      "999:\tlearn: 0.9197341\ttotal: 3.01s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 11360, number of negative: 15253\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000233 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 26613, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.426859 -> initscore=-0.294678\n",
      "[LightGBM] [Info] Start training from score -0.294678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 1 F1 Score  0.9317784256559768\n",
      "Fold 1 ROC AUC Score  0.9204232912237109\n",
      "Fold 1 Confusion matrix\n",
      "[[3111  529]\n",
      " [  56 3995]]\n",
      "\n",
      "\n",
      "Fold 2 train (2020-04-20, 2020-08-17) test (2020-08-18, 2020-09-16)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9087567\ttotal: 2.96ms\tremaining: 2.96s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9170544\ttotal: 1.39s\tremaining: 1.38s\n",
      "999:\tlearn: 0.9235379\ttotal: 2.99s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 10957, number of negative: 14063\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000202 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 25020, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.437930 -> initscore=-0.249569\n",
      "[LightGBM] [Info] Start training from score -0.249569\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 2 F1 Score  0.9369768586903005\n",
      "Fold 2 ROC AUC Score  0.9254382693024992\n",
      "Fold 2 Confusion matrix\n",
      "[[1464  244]\n",
      " [  12 1903]]\n",
      "\n",
      "\n",
      "Fold 3 train (2020-05-20, 2020-09-16) test (2020-09-17, 2020-10-16)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9216980\ttotal: 7.37ms\tremaining: 7.36s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9272288\ttotal: 1.44s\tremaining: 1.43s\n",
      "999:\tlearn: 0.9318023\ttotal: 2.94s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 11338, number of negative: 12467\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 23805, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.476286 -> initscore=-0.094925\n",
      "[LightGBM] [Info] Start training from score -0.094925\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 3 F1 Score  0.9019396551724137\n",
      "Fold 3 ROC AUC Score  0.9247331691297209\n",
      "Fold 3 Confusion matrix\n",
      "[[2078  358]\n",
      " [   6 1674]]\n",
      "\n",
      "\n",
      "Fold 4 train (2020-06-19, 2020-10-16) test (2020-10-17, 2020-11-15)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9165638\ttotal: 3.03ms\tremaining: 3.03s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9257521\ttotal: 1.72s\tremaining: 1.72s\n",
      "999:\tlearn: 0.9304316\ttotal: 3.31s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 12736, number of negative: 14441\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000382 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 27177, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.468632 -> initscore=-0.125639\n",
      "[LightGBM] [Info] Start training from score -0.125639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 4 F1 Score  0.86\n",
      "Fold 4 ROC AUC Score  0.9205051065611278\n",
      "Fold 4 Confusion matrix\n",
      "[[403  66]\n",
      " [  4 215]]\n",
      "\n",
      "\n",
      "Fold 5 train (2020-07-19, 2020-11-15) test (2020-11-16, 2020-12-14)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9175131\ttotal: 2.39ms\tremaining: 2.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9336425\ttotal: 1.12s\tremaining: 1.11s\n",
      "999:\tlearn: 0.9429429\ttotal: 2.26s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 7865, number of negative: 8253\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006774 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 16118, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.487964 -> initscore=-0.048154\n",
      "[LightGBM] [Info] Start training from score -0.048154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 5 F1 Score  0.7333333333333333\n",
      "Fold 5 ROC AUC Score  0.8007326007326008\n",
      "Fold 5 Confusion matrix\n",
      "[[79 12]\n",
      " [12 33]]\n",
      "\n",
      "\n",
      "Fold 6 train (2020-08-18, 2020-12-14) test (2020-12-19, 2021-01-05)\n",
      "Train catboost\n",
      "0:\tlearn: 0.9095508\ttotal: 6.63ms\tremaining: 6.62s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9318788\ttotal: 792ms\tremaining: 788ms\n",
      "999:\tlearn: 0.9510265\ttotal: 1.56s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 3859, number of negative: 4704\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000815 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 8563, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.450660 -> initscore=-0.198005\n",
      "[LightGBM] [Info] Start training from score -0.198005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 6 F1 Score  0.9\n",
      "Fold 6 ROC AUC Score  0.9142857142857143\n",
      "Fold 6 Confusion matrix\n",
      "[[13  1]\n",
      " [ 1  9]]\n",
      "\n",
      "\n",
      "Fold 7 train (2020-09-17, 2021-01-05) test (2021-02-04, 2021-02-12)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8896552\ttotal: 1.39ms\tremaining: 1.39s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9413760\ttotal: 589ms\tremaining: 587ms\n",
      "999:\tlearn: 0.9646302\ttotal: 1.2s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 1954, number of negative: 3010\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000354 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1075\n",
      "[LightGBM] [Info] Number of data points in the train set: 4964, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.393634 -> initscore=-0.432062\n",
      "[LightGBM] [Info] Start training from score -0.432062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fold 7 F1 Score  0.6666666666666666\n",
      "Fold 7 ROC AUC Score  0.8928571428571428\n",
      "Fold 7 Confusion matrix\n",
      "[[11  3]\n",
      " [ 0  3]]\n",
      "\n",
      "\n",
      "Fold 8 train (2020-10-17, 2021-02-12) test (2021-02-25, 2021-03-15)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8325509\ttotal: 654us\tremaining: 653ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9752650\ttotal: 258ms\tremaining: 257ms\n",
      "999:\tlearn: 0.9981982\ttotal: 513ms\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 277, number of negative: 588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001172 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 680\n",
      "[LightGBM] [Info] Number of data points in the train set: 865, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.320231 -> initscore=-0.752709\n",
      "[LightGBM] [Info] Start training from score -0.752709\n",
      "\n",
      "Fold 8 F1 Score  0.8228571428571428\n",
      "Fold 8 ROC AUC Score  0.8536427275084262\n",
      "Fold 8 Confusion matrix\n",
      "[[117  16]\n",
      " [ 15  72]]\n",
      "\n",
      "\n",
      "Fold 9 train (2020-11-16, 2021-03-15) test (2021-03-16, 2021-04-14)\n",
      "Train catboost\n",
      "0:\tlearn: 0.8662420\ttotal: 563us\tremaining: 563ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/pandas/core/ops/array_ops.py:73: FutureWarning: Comparison of Timestamp with datetime.date is deprecated in order to match the standard library behavior. In a future version these will be considered non-comparable. Use 'ts == pd.Timestamp(date)' or 'ts.date() == date' instead.\n",
      "  result = libops.scalar_compare(x.ravel(), y, op)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500:\tlearn: 0.9931507\ttotal: 207ms\tremaining: 206ms\n",
      "999:\tlearn: 1.0000000\ttotal: 412ms\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 145, number of negative: 252\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004959 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 327\n",
      "[LightGBM] [Info] Number of data points in the train set: 397, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.365239 -> initscore=-0.552695"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[LightGBM] [Info] Start training from score -0.552695\n",
      "\n",
      "Fold 9 F1 Score  0.7746031746031746\n",
      "Fold 9 ROC AUC Score  0.7998847677793457\n",
      "Fold 9 Confusion matrix\n",
      "[[1202  156]\n",
      " [ 341  854]]\n",
      "\n",
      "Overall OOF F1 Micro  0.9307062909109078\n",
      "Overall OOF Mean ROC AUC Score  0.9307062909109078\n"
     ]
    }
   ],
   "source": [
    "fire_model.train_folds(train_size=120, test_size=30, iterations=1000, early_stopping=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb645a6f-5d16-49d0-a1ab-57b183e1fad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train catboost\n",
      "0:\tlearn: 0.9153127\ttotal: 4.56ms\tremaining: 4.56s\n",
      "500:\tlearn: 0.9181814\ttotal: 2.76s\tremaining: 2.75s\n",
      "999:\tlearn: 0.9208589\ttotal: 4.98s\tremaining: 0us\n",
      "Train lgbm\n",
      "[LightGBM] [Info] Number of positive: 21278, number of negative: 25809\n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000357 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1079\n",
      "[LightGBM] [Info] Number of data points in the train set: 47087, number of used features: 9\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.451887 -> initscore=-0.193050\n",
      "[LightGBM] [Info] Start training from score -0.193050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/imd/miniconda3/envs/wildfires/lib/python3.8/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    }
   ],
   "source": [
    "cat_model, lgbm_model = fire_model.train_final_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca59767",
   "metadata": {},
   "source": [
    "### Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5890e0fb-4ed2-4fef-b7c3-eecdbe757160",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x7f6b55115d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_model.save_model(\"catboost\", format=\"cbm\")\n",
    "\n",
    "lgbm_model.save_model(\"light_gbm.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023a313-9d70-4142-94eb-0a068d82acce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
